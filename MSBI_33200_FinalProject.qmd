---
title: "MSBI 33200 - Final Project"
author:
  - Ankur Bhargava
  - Won Chang
  - Eliza Perchanok
date: August 10, 2022
bibliography: supporting_files/MSBI33200_FinalProject_Bibliography.bib
csl: supporting_files/apa.csl
execute: 
  echo: false
format: 
  html:
    page-layout: full
toc: true
theme: slate
---

# Preamble 

## Dataset Description 

These data were collected using questionnaires approved by a physician from the patients of Sylhet Diabetes Hospital in Sylhet, Bangladesh [@RN1].

## Project Description 

This project entails the development and elucidation of the best machine learning model that could potentially aid the more rapid detection of early-stage diabetes on the basis of patient-described symptoms. What will be demonstrated here can be thought of as the second step in a computer-aided diagnostic pipeline where the first was the use of Natural Language Processing (NLP) to extract key symptomatology from patient reports contained in clinical notes. 

# Background 

## Global Burden of T2DM

The incidence of Type 2 (non-insulin-dependent) diabetes (T2DM) has been increasing worldwide. According to the Global Burden of Disease Dataset, approximately 6.28% of the world's population (~462 million individuals) were afflicted with T2DM. In 2017, the estimated prevalence of T2DM was 6059 per 100,000 cases. Over 1 million deaths annually can be attributed solely to T2DM making it the 9th leading cause of mortality. Incidence of T2DM is rising globally, and at a faster rate in developed regions. By 2030, the estimated prevalence is projected to be 7079 per 100,000. Early detection and treatment of T2DM will be critical to mitigate this increase [@RN2].

Certain regions of the world, such as Pacific Ocean island nations, sustain the highest prevalence of disease. These countries include Fiji (20,277 per 100,000), Mauritius (18,545), American Samoa (18,312), and Kiribati (17,432). Southeast Asian countries, such as Indonesia, Malaysia, Thailand, and Vietnam, have moved up the ranks in the last two decades. Owing to their large population sizes, China (88.5 million individuals with type 2 diabetes), India (65.9 million), and the US (28.9 million) retain the top spots as the countries with the greatest total number of individuals with this condition [@RN1].  

**Forecasted Prevalence of T2DM**

![Forecasted Prevalence of T2DM](supporting_files/JEGH-10-1-107-g002.jpg)  
[@RN1]

## Pathophysiology of T2DM - from UpToDate

"Type 2 diabetes is by far the most common type of diabetes in adults (>90 percent) and is characterized by hyperglycemia usually due to progressive loss of insulin secretion from the beta cell superimposed on a background of insulin resistance, resulting in relative insulin deficiency. The majority of patients are asymptomatic at presentation, with hyperglycemia noted on routine laboratory evaluation, prompting further testing. The frequency of symptomatic diabetes has been decreasing in parallel with improved efforts to diagnose diabetes earlier through screening.

The classic symptoms of hyperglycemia (including polyuria, polydipsia, nocturia, blurred vision, and weight loss) are often noted only in retrospect after a blood glucose value has been shown to be elevated. Polyuria occurs when the serum glucose concentration rises significantly above 180 mg/dL (10 mmol/L), exceeding the renal threshold for glucose reabsorption, which leads to increased urinary glucose excretion. Glycosuria causes osmotic diuresis (i.e., polyuria) and hypovolemia, which in turn can lead to polydipsia. Patients who replete their volume losses with concentrated sugar drinks, such as non-diet sodas, exacerbate their hyperglycemia and osmotic diuresis.

Rarely adults with type 2 diabetes can present with a hyperosmolar hyperglycemic state, characterized by marked hyperglycemia, severe dehydration, and obtundation, but without ketoacidosis. Diabetic ketoacidosis (DKA) as the presenting symptom of type 2 diabetes is also uncommon in adults but may occur under certain circumstances (usually severe infection or other acute illness)" [@RN3].

## Rationale for Computer-Aided Prediction of Possible T2DM 

Individuals with pre-diabetes lose approximately 4.3 years of potential life. Individuals with diabetes lose nearly 7.9 years of potential life. The years of potential life lost (YPLL) increases linearly with increasing disease severity and associated comorbidities (e.g. congestive heart failure, limb amputation, blindness) [@RN4]. In the United States, approximately 16 million adults have undiagnosed T2DM, and an estimated $327 billion dollars in economic productivity was lost in 2017 due to T2DM [@RN9, @RN10]. It is therefore a worthwhile endeavor to speed up the early detection of this disorder and maintain normoglycemic states for as many individuals as possible.  

The risks of a false positive diagnosis of T2DM may include psychological distress and unnecessary employment of medications. The risks of a false negative diagnosis of T2DM, however, include that of premature death. There is existing precedence for the use of machine learning models for early diagnosis of conditions (e.g. Chronic Myelogenous Leukemia - CML) [@RN5], so we aim to create a similar predictive model here. 

# Methods 

- The data set used for creating the models is noted above. 
- Exploratory data analysis (EDA) will be performed. 
  - Missingness
  - Counts, Histograms
  - Correlogram
- ML Algorithms - Naive 
  - Logistic Regression - considered the "go-to" method for binary classification problems, such as this one [@RN6]. Application of L2 regularization by default.
  - Gaussian Naive Bayes [@RN7] 
  - XGBoost - Also considered a good choice for this kind of problem and data set [@RN8]. Training 100 trees (XGBoost Default). 
- Tools
  - R version 4.2.0 
    - Tidyverse
    - data.table
    - mltools
    - rcompanion
    - corrr
    - reticulate
  - Python version 3.9.12
    - Scikit-learn 
    - Numpy
    - Pandas

# Exploratory Data Analysis 

```{r}
#| label: setup 
#| include: false 
#| message: false
#| warning: false

library(tidyverse)
library(data.table)
library(mltools)
library(rcompanion)
library(corrr)
library(reticulate)
reticulate::use_condaenv("MSBI_33200") # Conda environment created specifically for this 

diabetes <- 
  fread("./supporting_files/diabetes_data_upload.csv") %>%
  rename(
    AGE = Age, 
    GENDER = Gender,
    POLYURIA = Polyuria, 
    POLYDIPSIA = Polydipsia,
    SUDDEN_WT_LOSS = `sudden weight loss`,
    WEAKNESS = weakness, 
    POLYPHAGIA = Polyphagia,
    GENITAL_THRUSH = `Genital thrush`,
    VIS_BLUR = `visual blurring`,
    ITCHING = Itching,
    IRRITABILITY = Irritability,
    IMPAIRED_HEALING = `delayed healing`,
    PARTIAL_PARESIS = `partial paresis`,
    MUSCLE_STIFFNESS = `muscle stiffness`,
    ALOPECIA = Alopecia,
    OBESITY = Obesity, 
    CLASS = class
  )
```

## Missingness Visualization 

```{r}
#| label: Missingness Visualization 
#| warning: false 

diabetes %>% naniar::vis_miss()
```

Nothing is missing - complete dataset. No imputation required. 

## Age Histogram 

```{r}
#| label: Age Histogram

diabetes %>%
  ggplot(aes(x = AGE)) + 
  geom_histogram(color = "white", binwidth = 5, closed = "left", breaks = seq(15,100,5)) + 
  scale_y_continuous(breaks = seq(0,100,5), name = "# of Individuals", expand = c(0,0), limits = c(0,100)) + 
  scale_x_continuous(breaks = seq(15,100,5), name = "Age Bins") + 
  ggtitle("Age Histogram - Diabetes Data")
```

Age appears to be normally distributed in this data set. 

## Gender Percentages 

```{r}
#| label: Gender Percentages

diabetes %>%
  count(GENDER) %>%
  mutate(PCT = n/sum(n)) %>%
  ggplot(aes(x = GENDER, y = PCT)) + geom_col() + 
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.1), labels = scales::percent_format(1L), expand = c(0,0)) + 
  ggtitle("Gender Percentages - Diabetes Data")
```

Gender also appears to be nearly equally distributed between Male and Female.

## Positive vs. Negative Exemplars 

```{r}
#| label: class-breakdown

diabetes %>%
  count(CLASS, name = "N_PATIENTS") %>%
  mutate(PCT = N_PATIENTS/sum(N_PATIENTS)) %>%
  mutate(CLASS = factor(CLASS, labels = c("Positive", "Negative"), ordered = TRUE)) %>%
  ggplot(aes(x = CLASS, y = PCT)) +
  geom_col() + 
  scale_y_continuous(name = "Percentage", breaks = seq(0,1,.1), limits = c(0,1), labels = scales::percent_format(1L)) + 
  xlab("Patient Class") + 
  ggtitle("Class Breakdown - Diabetes Data")
```

The classes are not terribly imbalanced, so a method like [SMOTE](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/) is not needed. 


## Eliza: Chi-Square Test 

### 2x2 Contingency Table 

```{r}
#| label: crosstab-thrush-pre-dm

diabetes %>%
  rename("Genital Thrush" = GENITAL_THRUSH) %>% 
  mutate(CLASS = fct_recode(CLASS, "No Pre-DM" = "Negative", "Pre-DM" = "Positive")) %>% 
  group_by(`Genital Thrush`, CLASS) %>% 
  summarize(n = n()) %>%
  spread(CLASS, n) %>%
  janitor::adorn_totals() %>%
  knitr::kable()
```

### Hypotheses 

$H_{0}:$ There is no difference in the reports of genital thrush between those with early-stage diabetes and those without. 

$H_{a}:$ There is a difference in the reports of genital thrush between those with early-stage diabetes and those without. 

### Chi-Square Test 

```{r}
#| label: chisquare-test-thrush-diabetes

chisq.test(diabetes$GENITAL_THRUSH, diabetes$CLASS)
```

**Conclusion:**    

Reject the null and state that there is a difference in the reports of genital thrush between those with early-stage diabetes and those without early-stage diabetes. 

## Correlogram 

[REFERENCE FOR HOW THIS WAS BUILT](https://stackoverflow.com/questions/52554336/plot-the-equivalent-of-correlation-matrix-for-factors-categorical-data-and-mi)

```{r}
#| label: correlogram function
#| code-fold: true

# Calculate a pairwise association between all variables in a data-frame. In particular nominal vs nominal with Chi-square, numeric vs numeric with Pearson correlation, and nominal vs numeric with ANOVA.
# Adopted from https://stackoverflow.com/a/52557631/590437
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
    df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")

    is_nominal = function(x) class(x) %in% c("factor", "character")
    # https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
    # https://github.com/r-lib/rlang/issues/781
    is_numeric <- function(x) { is.integer(x) || is_double(x)}

    f = function(xName,yName) {
        x =  pull(df, xName)
        y =  pull(df, yName)

        result = if(is_nominal(x) && is_nominal(y)){
            # use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
            cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
            data.frame(xName, yName, assoc=cv, type="cramersV")

        }else if(is_numeric(x) && is_numeric(y)){
            correlation = cor(x, y, method=cor_method, use="complete.obs")
            data.frame(xName, yName, assoc=correlation, type="correlation")

        }else if(is_numeric(x) && is_nominal(y)){
            # from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
            r_squared = summary(lm(x ~ y))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else if(is_nominal(x) && is_numeric(y)){
            r_squared = summary(lm(y ~x))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else {
            warning(paste("unmatched column type combination: ", class(x), class(y)))
        }

        # finally add complete obs number and ratio to table
        result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
    }

    # apply function to each variable combination
    map2_df(df_comb$X1, df_comb$X2, f)
}
```

```{r}
#| label: correlation-plot-network-diagram
#| code-fold: true
#| fig-cap: "Network Diagram Correlation Plot - Diabetes Dataset"
#| fig-cap-location: bottom

# tmaptools::palette_explorer() to get the color values below 

diabetes %>%
    select(-c(CLASS)) %>%
    mixed_assoc() %>%
    select(x, y, assoc) %>%
    spread(y, assoc) %>%
    column_to_rownames("x") %>%
    as.matrix %>%
    as_cordf %>%
    network_plot(colors = c("#00204D", "#7C7B78", "#FFEA46"), repel = TRUE)
```

The above correlogram displays the effect size (calculated with CramÃ©r's V). This is a measure of the strength of association between variables in a chi-square test [REFERENCE](https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=terms-cramrs-v). 

## Spearman Correlation Plot

```{r}
#| label: spearman-correlation-plot
#| code-fold: true
#| warning: false
#| results: hide 
#| fig-keep: none

# REF: https://stackoverflow.com/questions/62356378/spearman-correlation-plot-in-corrplot
library(corrplot)

diabetes %>% 
  mutate(across(where(is.character), as.factor)) %>%
  mutate(across(where(is.factor), as.numeric)) %>%
  mutate(across(2:17, ~ .x - 1)) %>%
  cor(.,method = "spearman") %>%
  corrplot(method = "color",
           type = "upper",
           addCoef.col = "black",
           col = viridis::cividis(n=100))
```

![Spearman Correlation Plot](supporting_files/spearman_corr_plot.PNG)  

The above demonstrates that polyuria, polydipsia, polyphagia, and sudden weight loss are all moderately correlated with presence of early-stage diabetes. 

# Logistic Regression in R 

```{r}
#| label: logistic-regression-r
#| results: hold

log_reg <- glm(CLASS ~ ., family = "binomial", data = diabetes %>% 
                 mutate(across(where(is.character), as.factor)) %>%
                 mutate(across(where(is.factor), as.numeric)) %>%
                 mutate(across(2:17, ~ .x - 1)))

summary(log_reg)
```

According to this logistic regression, the presence of polyuria, polydipsia, weight loss, genital thrush, itching, or irritability all increase the log odds of having early-stage diabetes.

```{r}
#| label: logistic-regression-graph
#| code-fold: true
#| message: false 
#| fig-cap: "Visualization of Logistic Regression"
#| fig-cap-location: bottom

diabetes %>%
  mutate(across(where(is.character), as.factor)) %>% 
  mutate(across(where(is.factor), as.numeric)) %>%
  mutate(across(2:17, ~ .x - 1)) %>% 
  ggplot() + 
  geom_point(aes(x = log_reg$fitted.values, y = CLASS)) + 
  geom_smooth(aes(x = log_reg$fitted.values, y = CLASS),
              method = "glm",
              method.args = list(family = "binomial"),
              se = TRUE,
              show.legend = TRUE) + 
  scale_y_continuous(name = "Predicted Class",
                     breaks = seq(0,1,1),
                     limits = c(0,1)) + 
  xlab("Logistic Regression Fitted Values") + 
  ggtitle("Manual Logistic Regression")
```

# Machine Learning in Python with R Data Preparation

```{r}
#| label: classifier-data-prep
#| warning: false

dm_onehot <- 
diabetes %>%
  mutate(across(where(is.character), as.factor)) %>%
  mltools::one_hot() %>%
  select(AGE, GENDER_Male, contains("_Yes"), CLASS_Positive) # Reduce multicollinearity 

head(dm_onehot,10) %>% 
  as.data.frame() %>% 
  knitr::kable()
```

```{python}
#| label: py-setup
#| code-fold: true
#| results: hold 
#| message: false
#| warning: false 

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay 
# REF: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions
import xgboost
import matplotlib.pyplot as plt 
import numpy as np
import pandas as pd 
```

## Load & Split Data into Train-Test 

```{python}
#| label: data-load-split
#| echo: true 
#| results: hold 

dm_one_hot = r.dm_onehot # the r. prefix allows for interfacing with the R environment and 'dragging' objects into Python
X = dm_one_hot.drop(['CLASS_Positive'], axis = 1)
y = dm_one_hot['CLASS_Positive']

(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size = 0.2, random_state = 0)
```

## Logistic Regression - Spawn/Fit/Predict

```{python}
#| label: spawn-fit-predict-logreg
#| echo: true 
#| results: hold 

# REF: LR Solvers - https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions

lr = LogisticRegression() # L2 penalty (Ridge) is applied by default 
lr_pred = lr.fit(X_train,y_train).predict(X_test)
lr_predict_proba = lr.predict_proba(X_test)
```

## Logistic Regression Metrics 

```{python}
#| label: confusion-matrix-logistic-regression
#| code-fold: true
#| message: false 
#| warning: false 
#| results: hold 

disp = ConfusionMatrixDisplay.from_predictions(y_test, lr_pred)
disp.plot(cmap=plt.cm.Blues)
plt.title("Logistic Regression Confusion Matrix")
plt.show()
```


```{python}
#| label: model-metrics-logistic-regression
#| code-fold: true
#| results: hold 

# REF: https://towardsdatascience.com/model-evaluation-in-scikit-learn-abce32ee4a99
print(f"Accuracy: {accuracy_score(y_test, lr_pred):.3f}\n"
      f"Precision: {precision_score(y_test,lr_pred):.3f}\n"
      f"Recall: {recall_score(y_test, lr_pred):.3f}\n"
      f"F1: {f1_score(y_test, lr_pred):.3f}\n"
      f"R^2: {r2_score(y_test,lr_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,lr_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,lr_pred)):.3f}")
```

## Gaussian Naive Bayes - Spawn/Fit/Predict 

```{python}
#| label: spawn-fit-predict-gnb
#| echo: true 
#| results: hold 

gnb = GaussianNB()
gnb_pred = gnb.fit(X_train, y_train).predict(X_test)
gnb_predict_proba = gnb.predict_proba(X_test)
```

## Gaussian Naive Bayes Metrics 

```{python}
#| label: confusion-matrix-gnb
#| code-fold: true
#| results: hold 

disp = ConfusionMatrixDisplay.from_predictions(y_test, gnb_pred)
disp.plot(cmap=plt.cm.Blues)
plt.title("Gaussian Naive Bayes Confusion Matrix")
plt.show()
```

```{python}
#| label: model-metrics-gnb
#| code-fold: true 
#| results: hold 

print(f"Accuracy: {accuracy_score(y_test, gnb_pred):.3f}\n"
      f"Precision: {precision_score(y_test,gnb_pred):.3f}\n"
      f"Recall: {recall_score(y_test, gnb_pred):.3f}\n"
      f"F1: {f1_score(y_test, gnb_pred):.3f}\n"
      f"R^2: {r2_score(y_test,gnb_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,gnb_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,gnb_pred)):.3f}")
```

## XGBoost - Spawn/Fit/Predict

```{python}
#| label: spawn-fit-predict-xgboostclassifier
#| echo: true 
#| results: hold
#| output: false 


# REF: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/
# REF: https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/ (for performance metrics)

# Model predictions on the test data 
xgb = xgboost.XGBClassifier(use_label_encoder = False)
# REF: Necessary? https://stackoverflow.com/questions/68766331/how-to-apply-predict-to-xgboost-cross-validation

# XGBoost - the default number of boosting rounds is 100 
evalset = [(X_train, y_train), (X_test, y_test)]
xgb = xgb.fit(X_train, y_train, eval_metric = 'logloss', eval_set = evalset)
xgb_pred = xgb.predict(X_test)
xgb_predict_proba = xgb.predict_proba(X_test)
```

## XGBoost Metrics 

XGBoost is the best model so we will present some additional information 

### Confusion Matrix & Metrics 

```{python}
#| label: confusion-matrix-xgb
#| code-fold: true
#| results: hold 

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, xgb_pred, labels=xgb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("XGBoost Confusion Matrix")
plt.show()
```

```{python}
#| label: model-metrics-xgb-test
#| code-fold: true
#| results: hold 

print(f"Test vs. Predicted - XGBoost:\n"
      f"Accuracy: {accuracy_score(y_test, xgb_pred):.3f}\n"
      f"Precision: {precision_score(y_test,xgb_pred):.3f}\n"
      f"Recall: {recall_score(y_test, xgb_pred):.3f}\n"
      f"F1: {f1_score(y_test, xgb_pred):.3f}\n"
      f"R^2: {r2_score(y_test,xgb_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,xgb_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,xgb_pred)):.3f}")
```

```{python}
#| label: model-metrics-xgb-train
#| code-fold: true
#| results: hold 

xgb_pred_train = xgb.fit(X_train, y_train).predict(X_train)
xgb_predict_proba_train = xgb.predict_proba(X_train)

print(f"Train vs. Predicted - XGBoost:\n"
      f"Accuracy: {accuracy_score(y_train, xgb_pred_train):.3f}\n"
      f"Precision: {precision_score(y_train,xgb_pred_train):.3f}\n"
      f"Recall: {recall_score(y_train, xgb_pred_train):.3f}\n"
      f"F1: {f1_score(y_train, xgb_pred_train):.3f}\n"
      f"R^2: {r2_score(y_train,xgb_pred_train):.3f}\n"
      f"MAE: {mean_absolute_error(y_train,xgb_pred_train):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_train,xgb_pred_train)):.3f}")
```

### Predicted vs. Actuals 

Below is the raw output where "Predicted" is what the model predicted, "Actual" is the value in the dataset, and "Match" is whether or not the prediction matched the actual. There is only one False value at row 55. 

```{python}
#| label: xgboost-misclassification-display
#| code-fold: true
#| results: hold 

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)

p_a = {
  'Predicted': xgb_pred,
  'Actual': y_test
}
df = pd.DataFrame(data = p_a).reset_index(drop = True)
df['Match'] = df['Predicted'] == df['Actual']

print("Display of the one missed classification")
df.iloc[54:57]
```

### Learning Curve 

```{python}
#| label: XGBoost Learning Curves
#| code-fold: true

# REF: https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/

xgb_perf_metrics = xgb.evals_result()
fig, ax = plt.subplots()
ax.plot(xgb_perf_metrics['validation_0']['logloss'], label = 'Train')
ax.plot(xgb_perf_metrics['validation_1']['logloss'], label = 'Test')
ax.axhline(y = 0, linestyle = '--', color = 'black', label = "Perfect Prediction")
ax.set_title("Train has lower performance vs. Test - desirable!")
ax.set(xlabel = 'Number of Training Examples ', ylabel = 'Log-Loss Score')
ax.legend()
plt.show(fig)
```

XGBoost is not overfitting on the training data given that a generalizability gap is noted in the learning curve. 

### Feature Importance 

```{python}
#| label: xgboost-feature-importance
#| code-fold: true

# REF: https://www.kaggle.com/code/prashant111/xgboost-k-fold-cv-feature-importance/notebook
plt.figure().set_size_inches(11,8.5)
plt.show(xgboost.plot_importance(xgb, title = "XGBoost - Feature Importance"))
```

I find it interesting that according to the model age and polydipsia are the two most important features. Clinically, regardless of age, we look for the classic "triad" of polydipsia, polyphagia, polyuria. 

### Visualization of Decision Trees 

```{python}
#| label: xgboost-decision-tree-worst
#| results: hold
#| echo: false
#| fig-keep: all
#| column: page 

# REF: https://machinelearningmastery.com/visualize-gradient-boosting-decision-trees-xgboost-python/
# From above link look at post by Frank July 9, 2018 in comments 

# The first ('worst') decision tree 
xgboost.plot_tree(xgb, num_trees = 0, rankdir = 'LR')
fig = plt.gcf()
fig.set_size_inches(11,8.5)
# plt.savefig('XGBoost_First_Decision_Tree.png', dpi = 300)
plt.show(fig)
```

The first decision tree uses all the available features to predict whether or not an individual has early-stage T2DM 

```{python}
#| label: xgboost-decision-tree-best
#| results: hold
#| echo: false
#| fig-keep: all
#| column: page 


# The last ('best') decision tree 
xgboost.plot_tree(xgb, num_trees = 99, rankdir = 'LR')
fig = plt.gcf()
fig.set_size_inches(11,8.5)
# plt.savefig('XGBoost_Last_Decision_Tree.png', dpi = 300)
plt.show(fig)
```

The final decision tree uses only irritability, male gender, and alopecia to predict whether or not an individual has early-stage T2DM.

I (AB) find this very interesting because the model is considering important a putative clinically-relevant skin manifestation of T2DM solely on the basis of individuals asking "Yes/No" questions [@RN11]!

# Model Evaluation Recap 

**<u>Interpretation</u>** 

Accuracy: How often the model is correct  
Precision: The proportion of time the model correctly predicts a positive  
Recall: The proportion of actual positives that were identified correctly  
(Accuracy, precision, and recall are all somewhat similar)  
F1 Score: A balance between precision and recall

[REFERENCE](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)

## Summary Table 

```{r}
#| label: summary-model-eval-table
#| code-fold: true
LABELS <- c("Accuracy","Precision","Recall", "F1", "R^2", "MAE", "RMSE")
LOG_REG_METRICS <- c(0.952, 0.954, 0.969, 0.961, 0.797, 0.048, 0.219)
GNB_METRICS <- c(0.933, 0.913, 0.984, 0.947, 0.716, 0.067, 0.259)
XGB_METRICS <- c(0.990, 1.000, 0.984, 0.992, 0.959, 0.010, 0.098)
cbind(LABELS, GNB_METRICS, LOG_REG_METRICS, XGB_METRICS) %>% knitr::kable()
```

## ROC Curves 

```{python}
#| label: ROC Curve Data
#| results: hold 
#| code-fold: true

# REF: https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_predict_proba[:,1]) # Logistic Regression
gnb_fpr, gnb_tpr, _ = roc_curve(y_test, gnb_predict_proba[:,1]) # Gaussian Naive Bayes
xgb_fpr, xgb_tpr, _  = roc_curve(y_test, xgb_predict_proba[:,1]) # XGBoost 

# Dumb (by chance) classifier - ns = no skill 
ns_probs = [0 for _ in range(len(y_test))]
ns_auc = roc_auc_score(y_test, ns_probs)
ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)
```

```{python}
#| label: ROC Curve Graphs 
#| code-overflow: wrap
#| code-fold: true
#| message: false
#| results: hide
#| fig-keep: all

fig, ax = plt.subplots()
fig.set_size_inches(11,8.5)
ax.plot(ns_fpr, ns_tpr, linestyle = '--', label = "No Skill")
ax.plot(lr_fpr, lr_tpr, marker='.', label = "Logistic Regression")
ax.plot(gnb_fpr, gnb_tpr, marker='.', label = "Gaussian Naive Bayes")
ax.plot(xgb_fpr, xgb_tpr, marker = '.', label = "XGBoost")
ax.set_title("Logistic Regression vs. Gaussian Naive Bayes vs. XGBoost - Diabetes Prediction")
ax.set(xlabel = 'False Positive Rate', ylabel = 'True Positive Rate')
ax.legend()
# plt.savefig('ROC_Curve.png', dpi = 300)
plt.show(fig)
```

# Conclusions 

The above exploration shows that an XGBoost-based machine learning model provides high accuracy and precision with respect to early detection/prediction of T2DM.  

Further directions: Many research groups have already performed variants of similar, next differentiating step would be including an NLP pipeline to generate the Kaggle dataset from clinical notes. 

# References

::: {#refs}
:::