---
title: "MSBI 33200 - Final Project"
date: August 10, 2022
author:
  - Ankur Bhargava
  - Won Chang
  - Eliza Perchanok
bibliography: supporting_files/MSBI33200_FinalProject_Bibliography.bib
csl: supporting_files/apa.csl
format: 
  revealjs:
    theme: black
---

## Approach 

- An ML model could aid in faster detection of early-stage T2DM 
- Here we present what can be thought of as Step 2 in a computer-aided diagnostic pipeline
- Step 1 of this pipeline would be NLP to extract patient-reported symptoms from clinical notes

## Disease Background 

- 2017: estimated T2DM prevalence 6059/100,000 
- 2030: estimated T2DM prevalence 7079/100,000 By 2030
- China, India, United States numbers 1, 2, 3 in the world, respectively, for T2DM prevalence 
[@RN2,@RN1]

## The Situation

![Forecasted Prevalence of T2DM](supporting_files/JEGH-10-1-107-g002.jpg) 

## So What? 

- 2017: Estimate $327 billion dollars lost over the 5 preceding years due to T2DM [@RN10]
- "T2DM maims, it doesn't kill"
  - Blindness
  - Limb amputation
  - Multi-system neuropathies
- Earlier intervention = better outcomes!

# Methods 

## Data & EDA 

- Kaggle Dataset: "Early Stage Diabetes Risk Prediction" - questionnaire conducted at Sylhet Hospital, Bangladesh
- Exploratory data analysis (EDA)
  - Missingness
  - Counts, Histograms
  - Correlogram
  
## ML Algorithms

- Logistic Regression - considered the "go-to" method for binary classification problems, such as this one [@RN6]
- Gaussian Naive Bayes [@RN7] 
- XGBoost - Also considered a good choice for this kind of problem and data set [@RN8]

## Software & Packages 

- R version 4.2.0 
  - Tidyverse
  - data.table
  - mltools
  - rcompanion
  - corrr
  - reticulate
- Python version 3.9.12
  - Scikit-learn 
  - Numpy
  - Pandas

# Exploratory Data Analysis 

```{r}
#| label: setup 
#| include: false 
#| message: false
#| warning: false

library(tidyverse)
library(data.table)
library(mltools)
library(rcompanion)
library(corrr)
library(reticulate)
reticulate::use_condaenv("MSBI_33200") # Conda environment created specifically for this 

diabetes <- 
  fread("./supporting_files/diabetes_data_upload.csv") %>%
  rename(
    AGE = Age, 
    GENDER = Gender,
    POLYURIA = Polyuria, 
    POLYDIPSIA = Polydipsia,
    SUDDEN_WT_LOSS = `sudden weight loss`,
    WEAKNESS = weakness, 
    POLYPHAGIA = Polyphagia,
    GENITAL_THRUSH = `Genital thrush`,
    VIS_BLUR = `visual blurring`,
    ITCHING = Itching,
    IRRITABILITY = Irritability,
    IMPAIRED_HEALING = `delayed healing`,
    PARTIAL_PARESIS = `partial paresis`,
    MUSCLE_STIFFNESS = `muscle stiffness`,
    ALOPECIA = Alopecia,
    OBESITY = Obesity, 
    CLASS = class
  )
```

## Missingness Visualization 

```{r}
#| label: Missingness Visualization 
#| warning: false 

diabetes %>% naniar::vis_miss()
```

## Age Histogram 

```{r}
#| label: Age Histogram

diabetes %>%
  ggplot(aes(x = AGE)) + 
  geom_histogram(color = "white", binwidth = 5, closed = "left", breaks = seq(15,100,5)) + 
  scale_y_continuous(breaks = seq(0,100,5), name = "# of Individuals", expand = c(0,0), limits = c(0,100)) + 
  scale_x_continuous(breaks = seq(15,100,5), name = "Age Bins") + 
  ggtitle("Age Histogram - Diabetes Data")
```

## Gender Percentages 

```{r}
#| label: Gender Percentages

diabetes %>%
  count(GENDER) %>%
  mutate(PCT = n/sum(n)) %>%
  ggplot(aes(x = GENDER, y = PCT)) + geom_col() + 
  scale_y_continuous(limits = c(0,1), breaks = seq(0,1,.1), labels = scales::percent_format(1L), expand = c(0,0)) + 
  ggtitle("Gender Percentages - Diabetes Data")
```

## Positive vs. Negative Exemplars 

```{r}
#| label: class-breakdown

diabetes %>%
  count(CLASS, name = "N_PATIENTS") %>%
  mutate(PCT = N_PATIENTS/sum(N_PATIENTS)) %>%
  mutate(CLASS = factor(CLASS, labels = c("Positive", "Negative"), ordered = TRUE)) %>%
  ggplot(aes(x = CLASS, y = PCT)) +
  geom_col() + 
  scale_y_continuous(name = "Percentage", breaks = seq(0,1,.1), limits = c(0,1), labels = scales::percent_format(1L)) + 
  xlab("Patient Class") + 
  ggtitle("Class Breakdown - Diabetes Data")
```

# Chi-Square Test 

## Contingency Table 

```{r}
#| label: crosstab-thrush-pre-dm

diabetes %>%
  rename("Genital Thrush" = GENITAL_THRUSH) %>% 
  mutate(CLASS = fct_recode(CLASS, "No Pre-DM" = "Negative", "Pre-DM" = "Positive")) %>% 
  group_by(`Genital Thrush`, CLASS) %>% 
  summarize(n = n()) %>%
  spread(CLASS, n) %>%
  janitor::adorn_totals() %>%
  knitr::kable()
```

## Hypotheses 

$H_{0}:$ There is no difference in the reports of genital thrush between those with early-stage diabetes and those without. 

$H_{a}:$ There is a difference in the reports of genital thrush between those with early-stage diabetes and those without. 

## Chi-Square Test 

```{r}
#| label: chisquare-test-thrush-diabetes

chisq.test(diabetes$GENITAL_THRUSH, diabetes$CLASS)
```

**Conclusion:**    

Reject the null and state that there is a difference in the reports of genital thrush between those with early-stage diabetes and those without early-stage diabetes. 

## Correlogram

```{r}
#| label: correlogram function
#| code-fold: true

# Calculate a pairwise association between all variables in a data-frame. In particular nominal vs nominal with Chi-square, numeric vs numeric with Pearson correlation, and nominal vs numeric with ANOVA.
# Adopted from https://stackoverflow.com/a/52557631/590437
mixed_assoc = function(df, cor_method="spearman", adjust_cramersv_bias=TRUE){
    df_comb = expand.grid(names(df), names(df),  stringsAsFactors = F) %>% set_names("X1", "X2")

    is_nominal = function(x) class(x) %in% c("factor", "character")
    # https://community.rstudio.com/t/why-is-purr-is-numeric-deprecated/3559
    # https://github.com/r-lib/rlang/issues/781
    is_numeric <- function(x) { is.integer(x) || is_double(x)}

    f = function(xName,yName) {
        x =  pull(df, xName)
        y =  pull(df, yName)

        result = if(is_nominal(x) && is_nominal(y)){
            # use bias corrected cramersV as described in https://rdrr.io/cran/rcompanion/man/cramerV.html
            cv = cramerV(as.character(x), as.character(y), bias.correct = adjust_cramersv_bias)
            data.frame(xName, yName, assoc=cv, type="cramersV")

        }else if(is_numeric(x) && is_numeric(y)){
            correlation = cor(x, y, method=cor_method, use="complete.obs")
            data.frame(xName, yName, assoc=correlation, type="correlation")

        }else if(is_numeric(x) && is_nominal(y)){
            # from https://stats.stackexchange.com/questions/119835/correlation-between-a-nominal-iv-and-a-continuous-dv-variable/124618#124618
            r_squared = summary(lm(x ~ y))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else if(is_nominal(x) && is_numeric(y)){
            r_squared = summary(lm(y ~x))$r.squared
            data.frame(xName, yName, assoc=sqrt(r_squared), type="anova")

        }else {
            warning(paste("unmatched column type combination: ", class(x), class(y)))
        }

        # finally add complete obs number and ratio to table
        result %>% mutate(complete_obs_pairs=sum(!is.na(x) & !is.na(y)), complete_obs_ratio=complete_obs_pairs/length(x)) %>% rename(x=xName, y=yName)
    }

    # apply function to each variable combination
    map2_df(df_comb$X1, df_comb$X2, f)
}
```

```{r}
#| label: correlation-plot-network-diagram
#| code-fold: true
#| fig-cap: "Network Diagram Correlation Plot - Diabetes Dataset"
#| fig-cap-location: bottom

# tmaptools::palette_explorer() to get the color values below 

diabetes %>%
    select(-c(CLASS)) %>%
    mixed_assoc() %>%
    select(x, y, assoc) %>%
    spread(y, assoc) %>%
    column_to_rownames("x") %>%
    as.matrix %>%
    as_cordf %>%
    network_plot(colors = c("#00204D", "#7C7B78", "#FFEA46"), repel = TRUE)
```

The above correlogram displays the effect size (calculated with Cram√©r's V). This is a measure of the strength of association between variables in a chi-square test [REFERENCE](https://www.ibm.com/docs/en/cognos-analytics/11.1.0?topic=terms-cramrs-v).

## Spearman Correlation Plot 

![Spearman Correlation Plot](supporting_files/spearman_corr_plot.PNG)  

## Logistic Regression in R 

```{r}
#| label: logistic-regression-r
#| results: hold

log_reg <- glm(CLASS ~ ., family = "binomial", data = diabetes %>% 
                 mutate(across(where(is.character), as.factor)) %>%
                 mutate(across(where(is.factor), as.numeric)) %>%
                 mutate(across(2:17, ~ .x - 1)))

summary(log_reg)
```

According to this logistic regression, the presence of polyuria, polydipsia, weight loss, genital thrush, itching, or irritability all increase the log odds of having early-stage diabetes.

## Logistic Regression Visualization 

```{r}
#| label: logistic-regression-graph
#| code-fold: true
#| message: false 
#| fig-cap: "Visualization of Logistic Regression"
#| fig-cap-location: bottom

diabetes %>%
  mutate(across(where(is.character), as.factor)) %>% 
  mutate(across(where(is.factor), as.numeric)) %>%
  mutate(across(2:17, ~ .x - 1)) %>% 
  ggplot() + 
  geom_point(aes(x = log_reg$fitted.values, y = CLASS)) + 
  geom_smooth(aes(x = log_reg$fitted.values, y = CLASS),
              method = "glm",
              method.args = list(family = "binomial"),
              se = TRUE,
              show.legend = TRUE) + 
  scale_y_continuous(name = "Predicted Class",
                     breaks = seq(0,1,1),
                     limits = c(0,1)) + 
  xlab("Logistic Regression Fitted Values") + 
  ggtitle("Manual Logistic Regression")
```

# Machine Learning in Python with R Data Preparation

## Data Preparation - One-hot encoding 

```{r}
#| label: classifier-data-prep
#| warning: false

dm_onehot <- 
diabetes %>%
  mutate(across(where(is.character), as.factor)) %>%
  mltools::one_hot() %>%
  select(AGE, GENDER_Male, contains("_Yes"), CLASS_Positive) # Reduce multicollinearity 

head(dm_onehot,10) %>% 
  as.data.frame() %>% 
  knitr::kable()
```

```{python}
#| label: py-setup
#| code-fold: true
#| results: hold 
#| message: false
#| warning: false 

from sklearnex import patch_sklearn
patch_sklearn()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import mean_absolute_error,r2_score,mean_squared_error
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay 
# REF: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_predictions
import xgboost
import matplotlib.pyplot as plt 
import numpy as np
import pandas as pd 
```

## Load & Split Data into Train-Test 

```{python}
#| label: data-load-split
#| echo: true 
#| results: hold 

dm_one_hot = r.dm_onehot # the r. prefix allows for interfacing with the R environment and 'dragging' objects into Python
X = dm_one_hot.drop(['CLASS_Positive'], axis = 1)
y = dm_one_hot['CLASS_Positive']

(X_train, X_test, y_train, y_test) = train_test_split(X, y, test_size = 0.2, random_state = 0)
```


# Logistic Regression 

## LR - Spawn/Fit/Predict

```{python}
#| label: spawn-fit-predict-logreg
#| echo: true 
#| results: hold 

# REF: LR Solvers - https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-definitions

lr = LogisticRegression() # L2 penalty (Ridge) is applied by default 
lr_pred = lr.fit(X_train,y_train).predict(X_test)
lr_predict_proba = lr.predict_proba(X_test)
```


## LR Confusion Matrix 

```{python}
#| label: confusion-matrix-logistic-regression
#| code-fold: true
#| message: false 
#| warning: false 
#| results: hold 

disp = ConfusionMatrixDisplay.from_predictions(y_test, lr_pred)
disp.plot(cmap=plt.cm.Blues)
plt.title("Logistic Regression Confusion Matrix")
plt.show()
```

## LR Model Statistics  

```{python}
#| label: model-metrics-logistic-regression
#| code-fold: true
#| results: hold 

# REF: https://towardsdatascience.com/model-evaluation-in-scikit-learn-abce32ee4a99
print(f"Accuracy: {accuracy_score(y_test, lr_pred):.3f}\n"
      f"Precision: {precision_score(y_test,lr_pred):.3f}\n"
      f"Recall: {recall_score(y_test, lr_pred):.3f}\n"
      f"F1: {f1_score(y_test, lr_pred):.3f}\n"
      f"R^2: {r2_score(y_test,lr_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,lr_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,lr_pred)):.3f}")
```

# Gaussian Naive Bayes 

## GNB - Spawn/Fit/Predict 

```{python}
#| label: spawn-fit-predict-gnb
#| echo: true 
#| results: hold 

gnb = GaussianNB()
gnb_pred = gnb.fit(X_train, y_train).predict(X_test)
gnb_predict_proba = gnb.predict_proba(X_test)
```

## GNB Confusion Matrix 

```{python}
#| label: confusion-matrix-gnb
#| code-fold: true
#| results: hold 

disp = ConfusionMatrixDisplay.from_predictions(y_test, gnb_pred)
disp.plot(cmap=plt.cm.Blues)
plt.title("Gaussian Naive Bayes Confusion Matrix")
plt.show()
```

## GNB Model Statistics 

```{python}
#| label: model-metrics-gnb
#| code-fold: true 
#| results: hold 

print(f"Accuracy: {accuracy_score(y_test, gnb_pred):.3f}\n"
      f"Precision: {precision_score(y_test,gnb_pred):.3f}\n"
      f"Recall: {recall_score(y_test, gnb_pred):.3f}\n"
      f"F1: {f1_score(y_test, gnb_pred):.3f}\n"
      f"R^2: {r2_score(y_test,gnb_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,gnb_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,gnb_pred)):.3f}")
```

# XGBoost 

## XGB - Spawn/Fit/Predict 

```{python}
#| label: spawn-fit-predict-xgboostclassifier
#| echo: true 
#| results: hold
#| output: false 


# REF: https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/
# REF: https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/ (for performance metrics)

# Model predictions on the test data 
xgb = xgboost.XGBClassifier(use_label_encoder = False)

# XGBoost - the default number of boosting rounds is 100 
evalset = [(X_train, y_train), (X_test, y_test)]
xgb = xgb.fit(X_train, y_train, eval_metric = 'logloss', eval_set = evalset)
xgb_pred = xgb.predict(X_test)
xgb_predict_proba = xgb.predict_proba(X_test)
```

## XGB Confusion Matrix 

```{python}
#| label: confusion-matrix-xgb
#| code-fold: true
#| results: hold 

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

cm = confusion_matrix(y_test, xgb_pred, labels=xgb.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=xgb.classes_)
disp.plot(cmap=plt.cm.Blues)
plt.title("XGBoost Confusion Matrix")
plt.show()
```

## XGB Model Statistics 

```{python}
#| label: model-metrics-xgb-test
#| code-fold: true
#| results: hold 

print(f"Test vs. Predicted - XGBoost:\n"
      f"Accuracy: {accuracy_score(y_test, xgb_pred):.3f}\n"
      f"Precision: {precision_score(y_test,xgb_pred):.3f}\n"
      f"Recall: {recall_score(y_test, xgb_pred):.3f}\n"
      f"F1: {f1_score(y_test, xgb_pred):.3f}\n"
      f"R^2: {r2_score(y_test,xgb_pred):.3f}\n"
      f"MAE: {mean_absolute_error(y_test,xgb_pred):.3f}\n"
      f"RMSE: {np.sqrt(mean_squared_error(y_test,xgb_pred)):.3f}")
```

## XGB - Predicted vs. Actuals 

Below is the raw output where "Predicted" is what the model predicted, "Actual" is the value in the dataset, and "Match" is whether or not the prediction matched the actual. There is only one False value at row 55. 

```{python}
#| label: xgboost-misclassification-display
#| code-fold: true
#| results: hold 

pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 150)

p_a = {
  'Predicted': xgb_pred,
  'Actual': y_test
}
df = pd.DataFrame(data = p_a).reset_index(drop = True)
df['Match'] = df['Predicted'] == df['Actual']

print("Display of the one missed classification")
df.iloc[54:57]
```

## XGB Learning Curve 

```{python}
#| label: XGBoost Learning Curves
#| code-fold: true

# REF: https://machinelearningmastery.com/tune-xgboost-performance-with-learning-curves/

xgb_perf_metrics = xgb.evals_result()
fig, ax = plt.subplots()
ax.plot(xgb_perf_metrics['validation_0']['logloss'], label = 'Train')
ax.plot(xgb_perf_metrics['validation_1']['logloss'], label = 'Test')
ax.axhline(y = 0, linestyle = '--', color = 'black', label = "Perfect Prediction")
ax.set_title("Train has lower performance vs. Test - desirable!")
ax.set(xlabel = 'Number of Training Examples ', ylabel = 'Log-Loss Score')
ax.legend()
plt.show(fig)
```

## XGB Feature Importance 

```{python}
#| label: xgboost-feature-importance
#| code-fold: true

# REF: https://www.kaggle.com/code/prashant111/xgboost-k-fold-cv-feature-importance/notebook
plt.figure().set_size_inches(11,8.5)
plt.show(xgboost.plot_importance(xgb, title = "XGBoost - Feature Importance"))
```

- Per model, age/polydipsia/alopecia (!!!) are the 3 most important features!
- Clinically, classic "triad" of polyuria/polydipsia/polyphagia 

# XGBoost Decision Trees 

XGBoost by default creates 100 decision trees 

## First (Worst) Decision Tree 

![](supporting_files/XGBoost_First_Decision_Tree.png)  

Using all features to make prediction 

## Last (Best) Decision Tree 

![](supporting_files/XGBoost_Last_Decision_Tree.png)  

## Commentary 

Using only irritability, male gender, and alopecia to predict early-stage T2DM.

Very interesting because the model is considering important a putative clinically-relevant skin manifestation of T2DM solely on the basis of individuals asking "Yes/No" questions [@RN11]!

# Model Evaluation Recap 

**<u>Interpretation</u>** 

Accuracy: How often the model is correct  
Precision: The proportion of time the model correctly predicts a positive  
Recall: The proportion of actual positives that were identified correctly  
(Accuracy, precision, and recall are all somewhat similar)  
F1 Score: A balance between precision and recall

[REFERENCE](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)

## Summary Table 

```{r}
#| label: summary-model-eval-table
#| code-fold: true
LABELS <- c("Accuracy","Precision","Recall", "F1", "R^2", "MAE", "RMSE")
LOG_REG_METRICS <- c(0.952, 0.954, 0.969, 0.961, 0.797, 0.048, 0.219)
GNB_METRICS <- c(0.933, 0.913, 0.984, 0.947, 0.716, 0.067, 0.259)
XGB_METRICS <- c(0.990, 1.000, 0.984, 0.992, 0.959, 0.010, 0.098)
cbind(LABELS, GNB_METRICS, LOG_REG_METRICS, XGB_METRICS) %>% knitr::kable()
```

## ROC Curves 

![](supporting_files/ROC_Curve.PNG)  

# Conclusions 

- XGBoost-based ML model provides high accuracy/precision WRT early T2DM detection/prediction.
- Further directions: Many research groups have already performed variants of similar, next differentiating step would be including an NLP pipeline to generate the Kaggle dataset from clinical notes. 

# References

::: {#refs}
:::
